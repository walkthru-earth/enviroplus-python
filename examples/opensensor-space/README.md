# OpenSensor Space - Edge Data Collection

Modern, efficient sensor data collection using DuckDB with automatic cloud sync capability.

## Quick Start

```bash
# Run the interactive installer
./install-opensensor-space.sh
```

The installer will guide you through:
1. Installing dependencies (UV, DuckDB, rclone)
2. Generating a unique station UUID
3. Configuring cloud storage (optional)
4. Setting up automatic startup

## Features

- **DuckDB 1.4.2 LTS Backend**: Memory-efficient, high-performance data buffering
- **Hive-Partitioned Parquet**: Industry-standard format for efficient querying
- **UUID v7 Station IDs**: Time-ordered globally unique identifiers (sortable, includes timestamp)
- **5-second readings**: More responsive than the original 1-second interval while still granular
- **15-minute batches**: Efficient storage compared to 5-minute batches (60-90% size reduction)
- **Edge Processing**: All data processing happens locally on the Raspberry Pi
- **Resilient Design**: Continues operation even if sensors fail
- **Optional Cloud Sync**: Automatic lightweight sync to any S3-compatible storage

## Directory Structure

```
opensensor-space/
├── install-opensensor-space.sh   # Smart installer script
├── opensensor_space.py           # Main data collection script
├── sync_to_storage.sh            # Cloud sync script (rclone)
├── config.env                    # Your configuration (generated by installer)
├── config.env.template           # Template for manual configuration
├── README.md                     # This file
├── systemd/                      # Systemd service files
│   ├── opensensor_space_systemd.service
│   ├── sync_timer.service
│   └── sync_timer.timer
└── scripts/                      # Additional utility scripts
```

## Configuration

After running `install.sh`, your configuration is stored in `config.env`:

```bash
# Station Configuration
STATION_ID=<your-uuid>            # Unique identifier

# Sensor Settings
READ_INTERVAL=5                   # Seconds between reads
BATCH_DURATION=900                # 15 minutes per batch

# Cloud Sync (optional)
SYNC_ENABLED=true
STORAGE_PROVIDER=s3
STORAGE_ENDPOINT=https://data.source.coop
STORAGE_REGION=us-west-2

# Storage bucket and prefix
# For Source Cooperative:
STORAGE_BUCKET=us-west-2.opendata.source.coop
STORAGE_PREFIX=youssef-harby/weather-station-realtime-parquet/parquet

# For AWS S3:
# STORAGE_BUCKET=my-bucket-name
# STORAGE_PREFIX=sensor-data/station-01

# Storage credentials
AWS_ACCESS_KEY_ID=your-key
AWS_SECRET_ACCESS_KEY=your-secret
```

## Manual Installation

If you prefer to install manually without the interactive installer:

### 1. Install Dependencies

```bash
# Install UV package manager
curl -LsSf https://astral.sh/uv/install.sh | sh

# Install Python dependencies
cd ../..  # Navigate to project root
uv pip install -e ".[opensensor-space]"

# Install rclone (for cloud sync)
curl https://rclone.org/install.sh | sudo bash
```

### 2. Configure

```bash
# Copy template and edit
cp config.env.template config.env
nano config.env

# Generate a UUID v7 for STATION_ID
python3 -c "from uuid_utils import uuid7; print(uuid7())"

# Or with Python 3.12+:
# python3 -c "import uuid; print(uuid.uuid7())"
```

### 3. Configure rclone (for cloud sync)

```bash
rclone config
```

Follow the prompts to set up your storage provider (S3, Backblaze B2, Google Cloud, etc.).

### 4. Test

```bash
# Test data collection
uv run python opensensor_space.py

# Test sync (if enabled)
./sync_to_storage.sh
```

## Cloud Sync Setup

### Supported Storage Providers

The sync system uses **rclone**, which supports 40+ cloud providers:

- **Source Cooperative** (S3-compatible, open data focused)
- **AWS S3**
- **Backblaze B2** (cost-effective)
- **Google Cloud Storage**
- **Wasabi** (fast, affordable)
- **DigitalOcean Spaces**
- And many more...

### Why rclone?

Compared to AWS CLI:
- ✅ 50% smaller binary
- ✅ Works with any cloud provider
- ✅ Built-in retry logic and error handling
- ✅ Efficient delta syncs (only uploads changes)
- ✅ Optional encryption
- ✅ Better for Raspberry Pi

### Bucket and Prefix Configuration

The sync script uses `STORAGE_BUCKET` and `STORAGE_PREFIX` from `config.env`:

**Source Cooperative example:**
```bash
STORAGE_BUCKET=us-west-2.opendata.source.coop
STORAGE_PREFIX=username/project-name/sensor-data
```

**AWS S3 example:**
```bash
STORAGE_BUCKET=my-sensor-bucket
STORAGE_PREFIX=production/station-01
```

**Backblaze B2 example:**
```bash
STORAGE_BUCKET=my-b2-bucket
STORAGE_PREFIX=sensors/weather-station
```

The final sync path will be: `rclone-remote:STORAGE_BUCKET/STORAGE_PREFIX`

### Sync Performance Configuration

Edit `sync_to_storage.sh` to customize sync behavior:

```bash
# Bandwidth limiting
--bwlimit 1M  # Limit to 1 MB/s

# Transfer settings (optimized for Raspberry Pi)
--transfers 4      # Parallel transfers
--checkers 8       # File checkers

# Timeout settings
--contimeout 60s
--timeout 300s
--retries 3
```

## Systemd Services

After installation, two services are available:

### Data Collection Service

```bash
# Start data collection
sudo systemctl start opensensor-space.service

# Enable on boot
sudo systemctl enable opensensor-space.service

# Check status
sudo systemctl status opensensor-space.service

# View logs
sudo journalctl -u opensensor-space.service -f
```

### Sync Timer

```bash
# Start sync timer
sudo systemctl start opensensor-sync.timer

# Enable automatic sync
sudo systemctl enable opensensor-sync.timer

# Check timer status
sudo systemctl status opensensor-sync.timer

# View sync logs
sudo journalctl -u opensensor-sync.service -f
```

## Data Output

Data is stored in Hive-partitioned Parquet format:

```
output/
└── station=<UUID>/
    └── year=2025/
        └── month=01/
            └── day=23/
                └── hour=14/
                    ├── data_1400.parquet  (14:00-14:14)
                    ├── data_1415.parquet  (14:15-14:29)
                    ├── data_1430.parquet  (14:30-14:44)
                    └── data_1445.parquet  (14:45-14:59)
```

### Partition Structure

- **station**: Your unique UUID v7
- **year/month/day/hour**: Timestamp components (Hive partitioning)
- **filename**: `data_HHMM.parquet` where HHMM indicates the 15-minute interval (00, 15, 30, 45)

This structure enables efficient querying:

```python
import duckdb

# Query specific time range (partition pushdown is automatic)
duckdb.sql("""
    SELECT * FROM 'output/**/*.parquet'
    WHERE year = 2025 AND month = 1 AND day = 23
    AND hour = 14
""").show()

# Query specific 15-minute interval using filename pattern
duckdb.sql("""
    SELECT * FROM 'output/**/hour=14/data_1415.parquet'
""").show()
```

## Monitoring

### Check Collection Status

```bash
# View real-time logs
tail -f ../../logs/opensensor_space.log

# Check errors
tail -f ../../logs/opensensor_space_error.log
```

### Check Sync Status

```bash
# View sync logs
tail -f ../../logs/sync.log

# Check for sync errors
tail -f ../../logs/sync_error.log
```

### Data Size Monitoring

```bash
# Local data size
du -sh ../../output/

# Remote data size (if synced)
rclone size s3remote:your-bucket/path
```

## Troubleshooting

### Service Won't Start

```bash
# Check service status and logs
sudo systemctl status opensensor-space.service
sudo journalctl -u opensensor-space.service -n 100

# Verify configuration
cat config.env
```

### Sync Fails

```bash
# Test rclone connection
rclone lsd s3remote:

# Dry run sync
rclone sync ../../output/ s3remote:bucket/path --dry-run -v

# Check credentials
rclone config show
```

### High Memory Usage

DuckDB is already optimized for low memory, but you can further reduce usage:

```python
# In opensensor_space.py, adjust batch size
BATCH_DURATION = 600  # Reduce to 10 minutes if needed
```

### Slow Performance

```bash
# Check system resources
htop

# Check I/O wait
iostat -x 1

# Reduce sync concurrency in sync_to_storage.sh
--transfers 2  # Reduce from 4
```

## Migration from Old opensensor-space-edge

If you're migrating from the original repository:

**No more:**
- ❌ pandas/fastparquet dependencies
- ❌ Latitude/longitude in data (use UUID instead)
- ❌ 1-second read interval (now 5 seconds)
- ❌ 5-minute batches (now 15 minutes)
- ❌ Manual partition management

**New features:**
- ✅ DuckDB 1.4.2 LTS for better performance
- ✅ UUID-based station identification
- ✅ Smart interactive installer
- ✅ Systemd service management
- ✅ rclone for efficient multi-cloud sync
- ✅ Monolithic repository (easier maintenance)

## Advanced Usage

### Custom Data Processing

The collected Parquet files can be processed with any tool that supports Parquet:

- **DuckDB**: SQL queries directly on Parquet
- **Python**: pandas, polars, pyarrow
- **R**: arrow package
- **Spark**: For large-scale processing
- **Cloud analytics**: Athena, BigQuery, Snowflake

### Direct Cloud Upload (Advanced)

Instead of local buffering + sync, you can modify `opensensor_space.py` to write directly to S3:

```python
# Configure DuckDB S3 extension
con.execute("INSTALL httpfs; LOAD httpfs;")
con.execute(f"SET s3_region='us-west-2';")
con.execute(f"SET s3_access_key_id='{key}';")
con.execute(f"SET s3_secret_access_key='{secret}';")

# Write directly to S3
partition_query = f"""
    COPY (...) TO 's3://bucket/path/...' (...)
"""
```

However, local buffering is more resilient to network issues.

## Daily Aggregation (Optional)

The original project included a daily aggregation pipeline. This is now **optional** since:
- 15-minute batches are already efficient
- DuckDB can query across raw files very quickly
- Minute-level aggregates may not be needed

If you want to maintain the aggregation pipeline, you can:
1. Keep it in a separate GitHub Actions workflow
2. Run it locally with cron
3. Use DuckDB directly for ad-hoc aggregations

## Support

- **GitHub Issues**: [enviroplus-python/issues](https://github.com/pimoroni/enviroplus-python/issues)
- **Documentation**: This README and [main README](../../README.md)
- **Open Sensor Network**: [opensensor.space](https://opensensor.space/)

## License

Apache 2.0 - See LICENSE file in repository root.
