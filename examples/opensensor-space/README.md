# OpenSensor Space - Edge Data Collection

Modern, efficient sensor data collection using DuckDB with automatic cloud sync capability.

## Quick Start

```bash
# Run the interactive installer
./install-opensensor-space.sh
```

The installer will guide you through:
1. Installing dependencies (UV, DuckDB, rclone)
2. Generating a unique station UUID
3. Configuring cloud storage (optional)
4. Setting up automatic startup

## Features

- **DuckDB 1.4.2 LTS Backend**: Memory-efficient, high-performance data buffering
- **Hive-Partitioned Parquet**: Industry-standard format for efficient querying
- **UUID v7 Station IDs**: Time-ordered globally unique identifiers (sortable, includes timestamp)
- **5-second readings**: More responsive than the original 1-second interval while still granular
- **15-minute batches**: Efficient storage compared to 5-minute batches (60-90% size reduction)
- **Edge Processing**: All data processing happens locally on the Raspberry Pi
- **Resilient Design**: Continues operation even if sensors fail
- **Optional Cloud Sync**: Automatic lightweight sync to any S3-compatible storage

## Directory Structure

```
opensensor-space/
├── install-opensensor-space.sh   # Smart installer script
├── fix-user.sh                   # Fix script for user credential issues
├── opensensor_space.py           # Main data collection script
├── sync_to_storage.sh            # Cloud sync script (rclone)
├── config.env                    # Your configuration (generated by installer)
├── config.env.template           # Template for manual configuration
├── README.md                     # This file
├── CONFIGURATION.md              # Complete variable reference
├── systemd/                      # Systemd service files
│   ├── opensensor_space_systemd.service
│   ├── sync_timer.service
│   └── sync_timer.timer
└── scripts/                      # Additional utility scripts
    └── generate_uuid.py          # UUID v7 generator
```

## Configuration

After running `install.sh`, your configuration is stored in `config.env`:

```bash
# Station Configuration
STATION_ID=<your-uuid>            # Unique identifier

# Sensor Settings
READ_INTERVAL=5                   # Seconds between reads
BATCH_DURATION=900                # 15 minutes per batch

# Cloud Sync (optional)
SYNC_ENABLED=true
STORAGE_PROVIDER=s3
STORAGE_ENDPOINT=https://data.source.coop
STORAGE_REGION=us-west-2

# Storage bucket and prefix
# For Source Cooperative:
STORAGE_BUCKET=us-west-2.opendata.source.coop
STORAGE_PREFIX=youssef-harby/weather-station-realtime-parquet/parquet

# For AWS S3:
# STORAGE_BUCKET=my-bucket-name
# STORAGE_PREFIX=sensor-data/station-01

# Storage credentials
AWS_ACCESS_KEY_ID=your-key
AWS_SECRET_ACCESS_KEY=your-secret
```

## Manual Installation

If you prefer to install manually without the interactive installer:

### 1. Install Dependencies

```bash
# Install UV package manager
curl -LsSf https://astral.sh/uv/install.sh | sh

# Install Python dependencies
cd ../..  # Navigate to project root
uv pip install -e ".[opensensor-space]"

# Install rclone (for cloud sync)
curl https://rclone.org/install.sh | sudo bash
```

### 2. Configure

```bash
# Copy template and edit
cp config.env.template config.env
nano config.env

# Generate a UUID v7 for STATION_ID
python3 -c "from uuid_utils import uuid7; print(uuid7())"

# Or with Python 3.12+:
# python3 -c "import uuid; print(uuid.uuid7())"
```

### 3. Configure rclone (for cloud sync)

**No configuration needed!** The sync script uses environment variables from `config.env` - rclone will automatically use your credentials without any config file.

### 4. Test

```bash
# Test data collection
uv run python opensensor_space.py

# Test sync (if enabled)
./sync_to_storage.sh
```

## Cloud Sync Setup

### Supported Storage Providers

The sync system uses **rclone**, which supports 40+ cloud providers:

- **Source Cooperative** (S3-compatible, open data focused)
- **AWS S3**
- **Backblaze B2** (cost-effective)
- **Google Cloud Storage**
- **Wasabi** (fast, affordable)
- **DigitalOcean Spaces**
- And many more...

### Why rclone?

Compared to AWS CLI:
- ✅ 50% smaller binary
- ✅ Works with any cloud provider
- ✅ Built-in retry logic and error handling
- ✅ Efficient delta syncs (only uploads changes)
- ✅ Optional encryption
- ✅ Better for Raspberry Pi
- ✅ Modern environment variable support (no config files needed!)

### Bucket and Prefix Configuration

The sync script uses `STORAGE_BUCKET` and `STORAGE_PREFIX` from `config.env`:

**Source Cooperative example:**
```bash
STORAGE_BUCKET=us-west-2.opendata.source.coop
STORAGE_PREFIX=username/project-name/sensor-data
```

**AWS S3 example:**
```bash
STORAGE_BUCKET=my-sensor-bucket
STORAGE_PREFIX=production/station-01
```

**Backblaze B2 example:**
```bash
STORAGE_BUCKET=my-b2-bucket
STORAGE_PREFIX=sensors/weather-station
```

The final sync path will be: `rclone-remote:STORAGE_BUCKET/STORAGE_PREFIX`

### Sync Performance Configuration

Edit `sync_to_storage.sh` to customize sync behavior:

```bash
# Bandwidth limiting
--bwlimit 1M  # Limit to 1 MB/s

# Transfer settings (optimized for Raspberry Pi)
--transfers 4      # Parallel transfers
--checkers 8       # File checkers

# Timeout settings
--contimeout 60s
--timeout 300s
--retries 3
```

## Systemd Services

After installation, two services are available:

### Data Collection Service

```bash
# Start data collection
sudo systemctl start opensensor-space.service

# Enable on boot
sudo systemctl enable opensensor-space.service

# Check status
sudo systemctl status opensensor-space.service

# View logs
sudo journalctl -u opensensor-space.service -f
```

### Sync Timer

```bash
# Start sync timer
sudo systemctl start opensensor-sync.timer

# Enable automatic sync
sudo systemctl enable opensensor-sync.timer

# Check timer status
sudo systemctl status opensensor-sync.timer

# View sync logs
sudo journalctl -u opensensor-sync.service -f
```

## Data Output

Data is stored in Hive-partitioned Parquet format:

```
output/
└── station=<UUID>/
    └── year=2025/
        └── month=01/
            └── day=23/
                ├── data_0000.parquet  (00:00-00:14)
                ├── data_0015.parquet  (00:15-00:29)
                ├── data_0030.parquet  (00:30-00:44)
                ├── data_0045.parquet  (00:45-00:59)
                ├── data_0100.parquet  (01:00-01:14)
                ...
                ├── data_1400.parquet  (14:00-14:14)
                ├── data_1415.parquet  (14:15-14:29)
                ├── data_1430.parquet  (14:30-14:44)
                ├── data_1445.parquet  (14:45-14:59)
                ...
                └── data_2345.parquet  (23:45-23:59)
```

### Partition Structure

- **station**: Your unique UUID v7
- **year/month/day**: Timestamp components (Hive partitioning)
- **filename**: `data_HHMM.parquet` where HHMM indicates the 15-minute interval (00, 15, 30, 45)

All 96 files for a day (24 hours × 4 intervals) are stored in the same directory, making it easy to query full days.

This structure enables efficient querying:

```python
import duckdb

# Query specific day (all 96 files automatically included)
duckdb.sql("""
    SELECT * FROM 'output/**/*.parquet'
    WHERE year = 2025 AND month = 1 AND day = 23
""").show()

# Query specific 15-minute interval using filename pattern
duckdb.sql("""
    SELECT * FROM 'output/**/day=23/data_1415.parquet'
""").show()

# Query specific hour (all 4 intervals)
duckdb.sql("""
    SELECT * FROM 'output/**/day=23/data_14*.parquet'
""").show()
```

## Monitoring

### Check Collection Status

```bash
# View real-time logs
tail -f ../../logs/opensensor_space.log

# Check errors
tail -f ../../logs/opensensor_space_error.log
```

### Check Sync Status

```bash
# View sync logs
tail -f ../../logs/sync.log

# Check for sync errors
tail -f ../../logs/sync_error.log
```

### Data Size Monitoring

```bash
# Local data size
du -sh ../../output/

# Remote data size (if synced)
rclone size s3remote:your-bucket/path
```

## Troubleshooting

### Service Won't Start - User Credentials Error

**Symptom:** Service fails with "Failed to determine user credentials: No such process"

**Cause:** Systemd service is configured for wrong username (e.g., `pi` instead of your actual username)

**Fix:** Run the fix script:
```bash
cd examples/opensensor-space
sudo ./fix-user.sh
```

This will automatically detect your username and update the systemd services.

### Service Won't Start - Other Issues

```bash
# Check service status and logs
sudo systemctl status opensensor-space.service
sudo journalctl -u opensensor-space.service -n 100

# Verify configuration
cat config.env
```

### Sync Fails

```bash
# Load config variables
cd examples/opensensor-space
source config.env

# Test rclone connection with environment variables
RCLONE_S3_ACCESS_KEY_ID="${AWS_ACCESS_KEY_ID}" \
RCLONE_S3_SECRET_ACCESS_KEY="${AWS_SECRET_ACCESS_KEY}" \
RCLONE_S3_ENDPOINT="${STORAGE_ENDPOINT}" \
RCLONE_S3_REGION="${STORAGE_REGION}" \
rclone lsd :s3:${STORAGE_BUCKET}

# Dry run sync
./sync_to_storage.sh  # Add --dry-run flag if needed

# Check your config.env credentials
cat config.env | grep -E 'AWS_|STORAGE_'
```

### High Memory Usage

DuckDB is already optimized for low memory, but you can further reduce usage:

```python
# In opensensor_space.py, adjust batch size
BATCH_DURATION = 600  # Reduce to 10 minutes if needed
```

### Slow Performance

```bash
# Check system resources
htop

# Check I/O wait
iostat -x 1

# Reduce sync concurrency in sync_to_storage.sh
--transfers 2  # Reduce from 4
```

## Migration from Old opensensor-space-edge

If you're migrating from the original repository:

**No more:**
- ❌ pandas/fastparquet dependencies
- ❌ Latitude/longitude in data (use UUID instead)
- ❌ 1-second read interval (now 5 seconds)
- ❌ 5-minute batches (now 15 minutes)
- ❌ Manual partition management

**New features:**
- ✅ DuckDB 1.4.2 LTS for better performance
- ✅ UUID-based station identification
- ✅ Smart interactive installer
- ✅ Systemd service management
- ✅ rclone for efficient multi-cloud sync
- ✅ Monolithic repository (easier maintenance)

## Advanced Usage

### Custom Data Processing

The collected Parquet files can be processed with any tool that supports Parquet:

- **DuckDB**: SQL queries directly on Parquet
- **Python**: pandas, polars, pyarrow
- **R**: arrow package
- **Spark**: For large-scale processing
- **Cloud analytics**: Athena, BigQuery, Snowflake

### Direct Cloud Upload (Advanced)

Instead of local buffering + sync, you can modify `opensensor_space.py` to write directly to S3:

```python
# Configure DuckDB S3 extension
con.execute("INSTALL httpfs; LOAD httpfs;")
con.execute(f"SET s3_region='us-west-2';")
con.execute(f"SET s3_access_key_id='{key}';")
con.execute(f"SET s3_secret_access_key='{secret}';")

# Write directly to S3
partition_query = f"""
    COPY (...) TO 's3://bucket/path/...' (...)
"""
```

However, local buffering is more resilient to network issues.

## Daily Aggregation (Optional)

The original project included a daily aggregation pipeline. This is now **optional** since:
- 15-minute batches are already efficient
- DuckDB can query across raw files very quickly
- Minute-level aggregates may not be needed

If you want to maintain the aggregation pipeline, you can:
1. Keep it in a separate GitHub Actions workflow
2. Run it locally with cron
3. Use DuckDB directly for ad-hoc aggregations

## Support

- **GitHub Issues**: [enviroplus-python/issues](https://github.com/pimoroni/enviroplus-python/issues)
- **Documentation**: This README and [main README](../../README.md)
- **Open Sensor Network**: [opensensor.space](https://opensensor.space/)

## License

Apache 2.0 - See LICENSE file in repository root.
